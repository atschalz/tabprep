{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a0d5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from proxy_models import TargetMeanClassifier, TargetMeanRegressor\n",
    "from utils import make_cv_function\n",
    "from sklearn.pipeline import Pipeline\n",
    "import openml\n",
    "import pandas as pd\n",
    "from sympy import Dummy, rem\n",
    "from utils import get_benchmark_dataIDs, get_metadata_df\n",
    "from ft_detection import FeatureTypeDetector\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fca194",
   "metadata": {},
   "source": [
    "### Code for in-depth duplicate investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb18d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml\n",
    "import pandas as pd\n",
    "from sympy import Dummy, rem\n",
    "from utils import get_benchmark_dataIDs, get_metadata_df\n",
    "from ft_detection import FeatureTypeDetector\n",
    "\n",
    "benchmark = \"TabArena\"  # or \"TabArena\", \"TabZilla\", \"Grinsztajn\"\n",
    "dataset_name = 'APS'  # 'Amazon_employee_access', 'nyc-taxi-green-dec-2016', 'adult', 'bank-marketing', 'credit-g', 'diabetes', 'german-credit', 'heart-disease', 'house-votes-84', 'iris', 'mushroom', 'wine-quality-red'\n",
    "\n",
    "tids, dids = get_benchmark_dataIDs(benchmark)  \n",
    "\n",
    "remaining_cols = {}\n",
    "\n",
    "batch_sizes = {}\n",
    "\n",
    "for tid, did in zip(tids, dids):\n",
    "    task = openml.tasks.get_task(tid)  # to check if the datasets are available\n",
    "    data = openml.datasets.get_dataset(did)  # to check if the datasets are available\n",
    "    # if dataset_name not in data.name:\n",
    "    #     continue\n",
    "    # print(f\"Processing dataset: {data.name} (ID: {tid})\")\n",
    "    # else:\n",
    "    #     break\n",
    "    print(data.name)\n",
    "    X, _, _, _ = data.get_data()\n",
    "    y = X[data.default_target_attribute]\n",
    "    X = X.drop(columns=[data.default_target_attribute])\n",
    "\n",
    "    if X.shape[0] < 5000 and X.shape[1] < 1000:\n",
    "        batch_sizes[data.name] = 128\n",
    "    elif X.shape[0] < 10000 and X.shape[1] > 1000:\n",
    "        batch_sizes[data.name] = 16\n",
    "    elif X.shape[0] > 10000 and X.shape[1] < 1000:\n",
    "        batch_sizes[data.name] = 16\n",
    "    else:\n",
    "        batch_sizes[data.name] = 32\n",
    "\n",
    "    \n",
    "#     if benchmark == \"Grinsztajn\" and X.shape[0]>10000:\n",
    "#         X = X.sample(10000, random_state=0)\n",
    "#         y = y.loc[X.index]\n",
    "\n",
    "#     if task.task_type == \"Supervised Classification\":\n",
    "#         target_type = \"binary\" if y.nunique() == 2 else \"multiclass\"\n",
    "#     else:\n",
    "#         target_type = 'regression'\n",
    "#     if target_type==\"multiclass\":\n",
    "#         # TODO: Fix this hack\n",
    "#         y = (y==y.value_counts().index[0]).astype(int)  # make it binary\n",
    "#         target_type = \"binary\"\n",
    "#     elif target_type==\"binary\" and y.dtype not in [\"int\", \"float\", \"bool\"]:\n",
    "#         y = (y==y.value_counts().index[0]).astype(int)  # make it numeric\n",
    "#     else:\n",
    "#         y = y.astype(float)\n",
    "#     print(data.name)\n",
    "#     print(pd.Series({r: pd.concat([X.round(r).map(lambda x: 0.0 if x == 0.0 else x),y], axis=1).duplicated().mean() for r in [0,1,2,3,4,5,6,7,8,9,10]}).sort_values(ascending=False))\n",
    "#     print(\"--\"*20)\n",
    "\n",
    "#     cv_func = make_cv_function(target_type=target_type)\n",
    "#     target_model = TargetMeanClassifier if target_type == \"binary\" else TargetMeanRegressor\n",
    "#     dummy_model = DummyClassifier(strategy='prior') if target_type == \"binary\" else DummyRegressor(strategy='mean')\n",
    "#     print(data.name)\n",
    "#     print(f\"DUMMY: {np.mean(cv_func(X, y, Pipeline([('model', dummy_model)])))}\")\n",
    "#     print(f\"DUP-PRED: {np.mean(cv_func(X.astype(str).sum(axis=1), y, Pipeline([('model', target_model())])))}\")\n",
    "#     print(f\"DUP-PRED-ROUND0: {np.mean(cv_func(X.round(0).astype(str).sum(axis=1), y, Pipeline([('model', target_model())])))}\")\n",
    "#     print(f\"DUP-PRED-ROUND1: {np.mean(cv_func(X.round(1).astype(str).sum(axis=1), y, Pipeline([('model', target_model())])))}\")\n",
    "#     print(f\"DUP-PRED-ROUND2: {np.mean(cv_func(X.round(2).astype(str).sum(axis=1), y, Pipeline([('model', target_model())])))}\")\n",
    "#     print(f\"DUP-PRED-ROUND3: {np.mean(cv_func(X.round(3).astype(str).sum(axis=1), y, Pipeline([('model', target_model())])))}\")\n",
    "#     print(f\"SUM: {np.mean(cv_func(X.sum(axis=1), y, Pipeline([('model', target_model())])))}\")\n",
    "#     print(f\"PROD: {np.mean(cv_func(X.prod(axis=1), y, Pipeline([('model', target_model())])))}\")\n",
    "#     print(\"--\"*20)\n",
    "\n",
    "\n",
    "\n",
    "# # anneal: 0=\n",
    "# # diamonds: 0=0.07\n",
    "# # hazelnut: 3: 0.22, 2: 0.99\n",
    "# # maternal_health_risk: 0=0.64\n",
    "# # physiochemical_protein: 0=0.056\n",
    "# # qsar-biodeg: 0=0.05\n",
    "# # wine_quality: slightly increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecbc013",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in X.columns:\n",
    "    a = X[col].round(3).value_counts()\n",
    "    b = X[col].round(3).map(lambda x: 0.0 if x == 0.0 else x).astype(str).value_counts()\n",
    "    print(f\"{col}: {a.shape} vs {b.shape}\")\n",
    "    if a.shape!=b.shape:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f71740",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(X.round(2).map(lambda x: 0.0 if x == 0.0 else x).astype(str).sum(axis=1),y).sort_values(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9725c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in range(6):\n",
    "    print(f\"{r}: {X.round(r).map(lambda x: 0.0 if x == 0.0 else x).astype(str).duplicated().mean():3f},{pd.concat([X.round(r).map(lambda x: 0.0 if x == 0.0 else x).astype(str),y], axis=1).duplicated().mean():3f}\")\n",
    "    print(f\"{r}: {X.round(r).duplicated().mean()}, {pd.concat([X.round(r),y], axis=1).duplicated().mean()}\")\n",
    "    print(\"--\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3194fb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from proxy_models import TargetMeanClassifier, TargetMeanRegressor\n",
    "from utils import make_cv_function\n",
    "cv_func = make_cv_function(target_type=target_type)\n",
    "print(np.mean(cv_func(X.round(2).astype(str).sum(axis=1), y, Pipeline([('model', TargetMeanClassifier())]))))\n",
    "\n",
    "# .82 if we round to 2 decimals\n",
    "# .85 if we round to 3 decimals\n",
    "X_curr = X.copy()\n",
    "for iter in range(X.shape[1]):\n",
    "    res = pd.Series({col: np.mean(cv_func(X_curr.drop(col,axis=1).round(3).astype(str).sum(axis=1), y, Pipeline([('model', TargetMeanClassifier())]))) for col in X_curr.columns})\n",
    "    best = res.idxmax()\n",
    "    print(best + f\": {res.max()}\")\n",
    "    X_curr = X_curr.drop(best, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085c93e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product, combinations\n",
    "\n",
    "X_curr = X.copy()\n",
    "col_set = []\n",
    "eligible_cols = set(X_curr.columns.tolist())\n",
    "\n",
    "for iter in range(X.shape[1]):\n",
    "    combs = list(product(eligible_cols, [0,1,2,3,4,5]))\n",
    "    res = pd.Series({col+\"__\"+f\"{r}\": np.mean(cv_func(pd.concat([X_curr[col_set],X_curr[col].round(r)],axis=1).astype(str).sum(axis=1), y, Pipeline([('model', TargetMeanClassifier())]))) for col,r in combs})\n",
    "    best = res.idxmax()\n",
    "    print(best + f\": {res.max()}\")\n",
    "    col_set.append(best)\n",
    "    eligible_cols.remove(best.split(\"__\")[0])  # remove the original column from eligible columns\n",
    "    X_curr[best.split(\"__\")[0]] = X_curr[best.split(\"__\")[0]].round(int(best.split(\"__\")[1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23905164",
   "metadata": {},
   "source": [
    "### Code for checking numerical precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced2cabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml\n",
    "import pandas as pd\n",
    "from sympy import rem\n",
    "from utils import get_benchmark_dataIDs, get_metadata_df\n",
    "from ft_detection import FeatureTypeDetector\n",
    "\n",
    "benchmark = \"TabArena\"  # or \"TabArena\", \"TabZilla\", \"Grinsztajn\"\n",
    "dataset_name = 'superconductivity'  # 'Amazon_employee_access', 'nyc-taxi-green-dec-2016', 'adult', 'bank-marketing', 'credit-g', 'diabetes', 'german-credit', 'heart-disease', 'house-votes-84', 'iris', 'mushroom', 'wine-quality-red'\n",
    "\n",
    "tids, dids = get_benchmark_dataIDs(benchmark)  \n",
    "\n",
    "remaining_cols = {}\n",
    "\n",
    "for tid, did in zip(tids, dids):\n",
    "    task = openml.tasks.get_task(tid)  # to check if the datasets are available\n",
    "    data = openml.datasets.get_dataset(did)  # to check if the datasets are available\n",
    "    # if data.name!=dataset_name:\n",
    "    #     continue\n",
    "    # else:\n",
    "    #     break\n",
    "    print(data.name)\n",
    "    X, _, _, _ = data.get_data()\n",
    "    y = X[data.default_target_attribute]\n",
    "    X = X.drop(columns=[data.default_target_attribute])\n",
    "    \n",
    "    if benchmark == \"Grinsztajn\" and X.shape[0]>10000:\n",
    "        X = X.sample(10000, random_state=0)\n",
    "        y = y.loc[X.index]\n",
    "\n",
    "    if task.task_type == \"Supervised Classification\":\n",
    "        target_type = \"binary\" if y.nunique() == 2 else \"multiclass\"\n",
    "    else:\n",
    "        target_type = 'regression'\n",
    "    if target_type==\"multiclass\":\n",
    "        # TODO: Fix this hack\n",
    "        y = (y==y.value_counts().index[0]).astype(int)  # make it binary\n",
    "        target_type = \"binary\"\n",
    "    elif target_type==\"binary\" and y.dtype not in [\"int\", \"float\", \"bool\"]:\n",
    "        y = (y==y.value_counts().index[0]).astype(int)  # make it numeric\n",
    "    else:\n",
    "        y = y.astype(float)\n",
    "\n",
    "    X_num = X.select_dtypes(include=[float])\n",
    "    X_num = X_num.loc[:,~(X_num.fillna(-9999999)==X_num.fillna(-9999999).astype(int)).all().values]\n",
    "    print(f\"FP-Numerical features in {data.name}: {X_num.shape[1]}/{X.shape[1]}\")\n",
    "    if X_num.shape[1] == 0:\n",
    "        print(f\"No numerical features in {data.name}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # IDEA for numerical resolution detection: Round the values to the value of highest difference between to neighbouring values\n",
    "    from decimal import Decimal\n",
    "\n",
    "    # def _precision(x):\n",
    "    #     # turn into a Decimal so scientific notation is handled,\n",
    "    #     # then look at the negative exponent\n",
    "    #     d = Decimal(str(x)).normalize()\n",
    "    #     return max(-d.as_tuple().exponent, 0)\n",
    "\n",
    "    # precisions = {}\n",
    "    # for col in X_num.select_dtypes(include=['float']).columns:\n",
    "    #     nonzero = X_num[col].dropna().loc[lambda s: s != 0]\n",
    "    #     if nonzero.empty:\n",
    "    #         precisions[col] = 0\n",
    "    #     else:\n",
    "    #         precisions[col] = nonzero.map(_precision).max()\n",
    "\n",
    "    # current_precision = pd.Series(precisions, dtype=int)\n",
    "\n",
    "    def count_decimals_np(x, ndigits=2):\n",
    "        # format with at most ndigits, no sci‐notation, no trailing zeros\n",
    "        s = np.format_float_positional(x, precision=ndigits, trim='-')\n",
    "        # ensure there’s a decimal point\n",
    "        if \".\" not in s:\n",
    "            return 0\n",
    "        return len(s.split(\".\", 1)[1])\n",
    "\n",
    "    precisions = {}\n",
    "    for col in X_num.select_dtypes(\"float\").columns:\n",
    "        nonzero = X_num[col].dropna().loc[lambda s: s != 0]\n",
    "        if nonzero.empty:\n",
    "            precisions[col] = 0\n",
    "        else:\n",
    "            precisions[col] = nonzero.map(lambda x: count_decimals_np(x, ndigits=10)).max()\n",
    "\n",
    "    current_precision = pd.Series(precisions, dtype=int)\n",
    "\n",
    "    min_diff = pd.Series({col: pd.Series(X_num[col].dropna().unique()).sort_values().diff().min() for col in X_num.columns if X_num[col].dtype in [float]})\n",
    "    min_diff_precision = min_diff.map(lambda x: count_decimals_np(x, ndigits=10))\n",
    "    # min_diff_precision = min_diff.map(_precision)\n",
    "    # min_diff_precision = min_diff.apply(lambda x: len(str(x).split('.')[1]))\n",
    "\n",
    "    red = pd.concat([min_diff_precision, current_precision], axis=1).diff(axis=1)[1].min()\n",
    "    red_col = pd.concat([min_diff_precision, current_precision], axis=1).diff(axis=1)[1].idxmin()\n",
    "\n",
    "    print(f'Highest reduction: {red} in {red_col}')\n",
    "\n",
    "    if data.name == 'superconductivity':\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6265bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_decimals_np(x, ndigits=2):\n",
    "    # format with at most ndigits, no sci‐notation, no trailing zeros\n",
    "    s = np.format_float_positional(x, precision=ndigits, trim='-')\n",
    "    # ensure there’s a decimal point\n",
    "    if \".\" not in s:\n",
    "        return 0\n",
    "    return len(s.split(\".\", 1)[1])\n",
    "\n",
    "precisions = {}\n",
    "for col in X_num.select_dtypes(\"float\").columns:\n",
    "    nonzero = X_num[col].dropna().loc[lambda s: s != 0]\n",
    "    if nonzero.empty:\n",
    "        precisions[col] = 0\n",
    "    else:\n",
    "        precisions[col] = nonzero.map(lambda x: count_decimals_np(x, ndigits=2)).max()\n",
    "\n",
    "current_precision = pd.Series(precisions, dtype=int)\n",
    "current_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef70e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_up_np(arr, decimals=0):\n",
    "    factor = 10.0 ** decimals\n",
    "    return np.ceil(arr * factor) / factor\n",
    "# IDEA for numerical resolution detection: Round the values to the value of highest difference between to neighbouring values\n",
    "max_diff = pd.Series({col: X[col].sort_values().diff().max() for col in X.columns}).sort_values(ascending=False)\n",
    "round_up_np(max_diff)\n",
    "\n",
    "# IDEA for numerical resolution detection: Round the values to the value of highest difference between to neighbouring values\n",
    "current_precision = pd.Series({col: int(X[col].apply(lambda x: len(str(x).split('.')[1].rstrip('0'))).max()) for col in X.columns if X[col].dtype in [float]})\n",
    "\n",
    "min_diff = pd.Series({col: pd.Series(X[col].unique()).sort_values().diff().min() for col in X.columns if X[col].dtype in [float]})\n",
    "min_diff_precision = min_diff.apply(lambda x: len(str(x).split('.')[1].rstrip('0')))\n",
    "\n",
    "red = pd.concat([min_diff_precision, current_precision], axis=1).diff(axis=1)[1].min()\n",
    "red_col = pd.concat([min_diff_precision, current_precision], axis=1).diff(axis=1)[1].idxmin()\n",
    "\n",
    "print(f'Highest reduction: {red} in {red_col}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4952c923",
   "metadata": {},
   "source": [
    "### Combining all proxy models and learning from them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e6be87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import LeaveOneOutEncoder, TargetEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from utils import *\n",
    "for col in [col]:\n",
    "    nunique = X[col].nunique()\n",
    "    x = X[col]\n",
    "    print(f\"Column: {col} with {X[col].nunique()} unique values\")\n",
    "    if target_type == 'binary':\n",
    "        metric = roc_auc_score\n",
    "        print('Linear: ', metric(y, UnivariateLogisticClassifier().fit(X[col].astype(float).fillna(0).to_frame(), y).predict_proba(X[col].astype(float).fillna(0).to_frame())[:,1]))\n",
    "        print('Poly-2: ', metric(y, PolynomialLogisticClassifier(degree=2).fit(X[col].astype(float).fillna(0).to_frame(), y).predict_proba(X[col].astype(float).fillna(0).to_frame())[:,1]))\n",
    "        print('Poly-3: ', metric(y, PolynomialLogisticClassifier(degree=3).fit(X[col].astype(float).fillna(0).to_frame(), y).predict_proba(X[col].astype(float).fillna(0).to_frame())[:,1]))\n",
    "    else:\n",
    "        metric = root_mean_squared_error\n",
    "        print('Linear: ', metric(y, UnivariateLinearRegressor().fit(X[col].astype(float).fillna(0).to_frame(), y).predict(X[col].astype(float).fillna(0).to_frame())))\n",
    "        print('Poly-2: ', metric(y, PolynomialRegressor(degree=2).fit(X[col].astype(float).fillna(0).to_frame(), y).predict(X[col].astype(float).fillna(0).to_frame())))\n",
    "        print('Poly-3: ', metric(y, PolynomialRegressor(degree=3).fit(X[col].astype(float).fillna(0).to_frame(), y).predict(X[col].astype(float).fillna(0).to_frame())))\n",
    "    print('LGB Binning')\n",
    "    for m_bin in [2,4,8,12,16,32,64,128,256,512,1024,2048,min([nunique-100,4096])]:\n",
    "        if m_bin <= nunique and m_bin > 1:\n",
    "            pipe = Pipeline([\n",
    "                ('binner', LightGBMBinner(max_bin=m_bin)),\n",
    "                ('model', TargetMeanClassifier()) if target_type == 'binary' else ('model', TargetMeanRegressor())\n",
    "            ])\n",
    "            print(f\"Bins: {m_bin}, Score: {np.mean(detector.cv_scores_with_early_stopping(X[col].to_frame(), y, pipe))}\")\n",
    "    \n",
    "    print('Kmeans Binning')\n",
    "    for m_bin in [2,4,8,12,16,32,64,128,256,512,1024,2048,min([nunique-100,4096])]:\n",
    "        if m_bin <= nunique and m_bin > 1:\n",
    "            pipe = Pipeline([\n",
    "                ('KMeans-binner', KMeansBinner(max_bin=m_bin)),\n",
    "                ('model', TargetMeanClassifier()) if target_type == 'binary' else ('model', TargetMeanRegressor())\n",
    "            ])\n",
    "            print(f\"Bins: {m_bin}, Score: {np.mean(detector.cv_scores_with_early_stopping(X[col].to_frame(), y, pipe))}\")\n",
    "\n",
    "    print('Leave-One-Out: ', metric(y, LeaveOneOutEncoder().fit_transform(X[col].astype('category'), y)[col]))\n",
    "    print('Target Encoding: ', metric(y, TargetEncoder(min_samples_leaf=1, smoothing=1e-10).fit_transform(X[col].astype('category'), y)[col]))\n",
    "    \n",
    "    print('--'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0efe46",
   "metadata": {},
   "source": [
    "### Finding a good threshold for categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d572c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {}\n",
    "for col in X.columns:\n",
    "    stats[col] = get_optimal_cat_thresh(X[col], y, target_type='regression', verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5491dd",
   "metadata": {},
   "source": [
    "### Notes on cat detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a729c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still mistakes: \n",
    "# concrete_compressive_strength: Almost all are detected as categorical, but they are numerical\n",
    "# credit_card_clients_default: BILL_AMT1 found cat\n",
    "# customer_satisfaction_in_airline: Several missed categorical columns (likely low cardinality), Age found as cat\n",
    "# Food_Delivery_Time: Type_of_order and Type_of_Vehicle found as cat\n",
    "# heloc: One categorical found\n",
    "# houses: Age found to be cat\n",
    "# HR_Analytics_Job_Change_of_Data_Scientists: city_development_index found as cat (interaction test does not yield results, as there are almost no other numerical features)\n",
    "# kddcup09_appetency: some false discoveries & missed ones\n",
    "# (!) maternal_health_risk: Several categoricals although all are numerical - multiclass might be an issue!\n",
    "# online_shoppers_intention: One missed categorical, several found, Region found to be irrelevant\n",
    "# physiochemical_protein: Several false categoricals\n",
    "# APSFailure: Several false categoricals\n",
    "# Bioresponse: Several false categoricals\n",
    "# qsar-biodeg: One missed, one false categorical\n",
    "# students_dropout_and_academic_success: Several false categoricals\n",
    "# MIC: Several missed\n",
    "# superconductivity: Several (false?) categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77ccab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_func = make_cv_scores_with_early_stopping(target_type=target_type,)\n",
    "# Index(['BlastFurnaceSlag', 'Water', 'CoarseAggregate', 'FineAggregate', 'Age'], dtype='object')  \n",
    "col = X.columns[0]\n",
    "\n",
    "X_num = X.astype(float)\n",
    "\n",
    "for ncol, col in enumerate(['RESOURCE']): # enumerate(X.columns):\n",
    "    print(col)\n",
    "    interaction_performances = {}\n",
    "    interaction_significances = {}\n",
    "\n",
    "    ### 1. Get Interactions\n",
    "    cols_use = X_num.drop(col,axis=1).columns\n",
    "    f_0 = X_num[col]\n",
    "\n",
    "    features = {}\n",
    "    for num, col2 in enumerate(cols_use):\n",
    "        features[f\"{col}_/_{col2}\"] = X_num[col] / X_num[col2].replace(0, np.nan)\n",
    "        features[f\"{col}_x_{col2}\"] = X_num[[col, col2]].prod(axis=1)\n",
    "        features[f\"{col}_-_{col2}\"] = X_num[col].astype(np.float32) - X_num[col2].astype(np.float32)\n",
    "        features[f\"{col}_+_{col2}\"] = X_num[[col,col2]].sum(axis=1)\n",
    "\n",
    "    X_int = pd.DataFrame(features)\n",
    "\n",
    "    ### 2. Get Highest Correlation\n",
    "    corr_X_int = X_int.corrwith(y, method='spearman').abs().sort_values(ascending=False)\n",
    "\n",
    "    # Basic method\n",
    "    #highest_corr = X_int.corrwith(y, method='spearman').abs().sort_values(ascending=False).index[0]\n",
    "\n",
    "    # base_cors = X_num.corrwith(y).abs().sort_values(ascending=False)\n",
    "    # int_cors = X_int.corrwith(y, method='spearman').abs().to_frame()\n",
    "    # int_cors_diff = int_cors.apply(lambda x: float([x-base_cors.loc[x.name.split(f\"_{i}_\")].max() for i in ['+', '-', '/', 'x'] if len(x.name.split(f\"_{i}_\"))>1][0][0]),axis=1)\n",
    "    # highest_corr = int_cors_diff.sort_values(ascending=False).index[2]\n",
    "\n",
    "    for highest_corr in corr_X_int.index:\n",
    "\n",
    "        col2 = [highest_corr.split(f\"_{i}_\")[1] for i in ['+', '-', '/', 'x'] if len(highest_corr.split(f\"_{i}_\"))>1][0]\n",
    "        x = X_int[highest_corr]\n",
    "        base_performance = pd.Series({col: np.mean(detector.scores[col]['mean']) for col in [col,col2]})\n",
    "        stronger_col = base_performance.idxmax()\n",
    "        print(base_performance)\n",
    "\n",
    "        ### 3. Get regular, binned and polynomial performance of the interaction feature\n",
    "        # Regular TE performance\n",
    "        if target_type == \"regression\":\n",
    "            interaction_performances['mean'] = cv_func(x, y, Pipeline(steps=[('model', TargetMeanRegressor())]))\n",
    "        else:\n",
    "            interaction_performances['mean'] = cv_func(x, y, Pipeline(steps=[('model', TargetMeanClassifier())]))\n",
    "        print(f\"{x.name}: {np.mean(interaction_performances['mean']).round(2)}\")\n",
    "\n",
    "        # # Binned performance\n",
    "        for bins in [16, 32, 64, 128, 256]:\n",
    "            pipe = Pipeline([\n",
    "                (\"bin\", LightGBMBinner(bins)),\n",
    "                (\"model\", TargetMeanRegressor()) if target_type == \"regression\" else (\"model\", TargetMeanClassifier())\n",
    "            ])\n",
    "            interaction_performances[f'binned{bins}'] = cv_func(x.to_frame(), y, pipe)\n",
    "            print(f\"{x.name}-binned{bins}: {np.mean(interaction_performances[f'binned{bins}']).round(2)}\")\n",
    "\n",
    "            interaction_significances[f'interaction_binned{bins}_over_basecols'] = p_value_wilcoxon_greater_than_zero(\n",
    "                                    interaction_performances[f'binned{bins}'] - detector.scores[stronger_col]['mean']\n",
    "            )\n",
    "\n",
    "\n",
    "        # Polynomial performance\n",
    "        pipe = Pipeline([\n",
    "        (\"standardize\", QuantileTransformer(n_quantiles=np.min([x.nunique(), 1000]), random_state=42)),\n",
    "        (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "        # (\"standardize\", StandardScaler()),\n",
    "        (\"model\", PolynomialRegressor(3)) if target_type == \"regression\" else (\"model\", PolynomialLogisticClassifier(3))\n",
    "            ])\n",
    "        interaction_performances['poly3'] = cv_func(x.to_frame(), y, pipe)\n",
    "        print(f\"{x.name}-poly3: {np.mean(interaction_performances['poly3']).round(2)}\")\n",
    "\n",
    "\n",
    "        ### 4. Test whether the interaction improves over the original features and whether it itself is numeric\n",
    "        interaction_significances[f'interaction_mean_over_basecols'] = p_value_wilcoxon_greater_than_zero(\n",
    "                                interaction_performances['mean'] - detector.scores[stronger_col]['mean']\n",
    "        )\n",
    "\n",
    "\n",
    "        interaction_significances[f'interaction_poly3_over_basecols'] = p_value_wilcoxon_greater_than_zero(\n",
    "                                interaction_performances['poly3'] - detector.scores[stronger_col]['mean']\n",
    "        )\n",
    "\n",
    "        ### 5. Test whether categorical interaction is better than numerical interaction\n",
    "        # Combine interaction performance\n",
    "        x = X[col].astype(str) + X_num[col2].astype(str)\n",
    "        if target_type == \"regression\":\n",
    "            interaction_performances['combine'] = cv_func(x, y, Pipeline(steps=[('model', TargetMeanRegressor())]))\n",
    "        else:\n",
    "            interaction_performances['combine'] = cv_func(x, y, Pipeline(steps=[('model', TargetMeanClassifier())]))\n",
    "        print(f\"{col}&{col2}: {np.mean(interaction_performances['combine']).round(2)}\")\n",
    "\n",
    "        # # 5. Get groupby performance\n",
    "\n",
    "        for stat in ['mean']: #, 'std', 'min', 'max', 'median']:\n",
    "            if stat == 'mean':\n",
    "                value_map = X_num[col2].groupby(X[col]).mean()\n",
    "            elif stat == 'std':\n",
    "                value_map = X_num[col2].groupby(X[col]).std()\n",
    "            elif stat == 'min':\n",
    "                value_map = X_num[col2].groupby(X[col]).min()\n",
    "            elif stat == 'max':\n",
    "                value_map = X_num[col2].groupby(X[col]).max()\n",
    "            elif stat == 'median':\n",
    "                value_map = X_num[col2].groupby(X[col]).median()\n",
    "\n",
    "            x = X[col].map(value_map)\n",
    "            if target_type == \"regression\":\n",
    "                interaction_performances[f'{col2}By{col}-{stat}'] = cv_func(x, y, Pipeline(steps=[('model', TargetMeanRegressor())]))\n",
    "            else:\n",
    "                interaction_performances[f'{col2}By{col}-{stat}'] = cv_func(x, y, Pipeline(steps=[('model', TargetMeanClassifier())]))\n",
    "            print(f\"{col2}By{col}-{stat}: {np.mean(interaction_performances[f'{col2}By{col}-{stat}']).round(2)}\")\n",
    "\n",
    "            # # Binned performance\n",
    "            for bins in [16, 32, 64, 128, 256]:\n",
    "                pipe = Pipeline([\n",
    "                    (\"bin\", LightGBMBinner(bins)),\n",
    "                    (\"model\", TargetMeanRegressor()) if target_type == \"regression\" else (\"model\", TargetMeanClassifier())\n",
    "                ])\n",
    "                interaction_performances[f'{col2}By{col}-{stat}-binned{bins}'] = cv_func(x.to_frame(), y, pipe)\n",
    "                print(f\"{col2}By{col}-{stat}-binned{bins}: {np.mean(interaction_performances[f'{col2}By{col}-{stat}-binned{bins}']).round(2)}\")\n",
    "\n",
    "            # Polynomial performance\n",
    "            # pipe = Pipeline([\n",
    "            # (\"standardize\", QuantileTransformer(n_quantiles=np.min([x.nunique(), 1000]), random_state=42)),\n",
    "            # (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "            # # (\"standardize\", StandardScaler()),\n",
    "            # (\"model\", PolynomialRegressor(3)) if target_type == \"regression\" else (\"model\", PolynomialLogisticClassifier(3))\n",
    "            #     ])\n",
    "            # interaction_performances[f'{col2}By{col}-poly3'] = cv_func(x.to_frame(), y, pipe)\n",
    "            # print(f\"{col2}By{col}-poly3: {np.mean(interaction_performances[f'{col2}By{col}-poly3']).round(2)}\")\n",
    "\n",
    "            # for bins in [2, 4, 8, 16, 32, 64, 128]:\n",
    "\n",
    "            #     dt = DecisionTreeRegressor(\n",
    "            #         max_leaf_nodes = bins,     # desired number of bins\n",
    "            #         min_samples_leaf = 1,   # min pts per bin to avoid overfitting\n",
    "            #         # criterion = \"entropy\"   # or \"gini\"\n",
    "            #     )\n",
    "            #     X_int[highest_corr]\n",
    "\n",
    "            #     print(f\"Bins: {bins}, CV Score: {np.mean(cv_func(X_int[highest_corr].to_frame(), y, Pipeline(steps=[('model', dt)])))}\")\n",
    "\n",
    "\n",
    "    # if ncol==0:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9617ce28",
   "metadata": {},
   "source": [
    "### TargetMean with cut stuff - probably already covered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdf733b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def drop_infrequent_values(series, thresh=1):\n",
    "    value_counts = series.value_counts()\n",
    "    non_unique_values = value_counts[value_counts > thresh].index\n",
    "    return series[series.isin(non_unique_values)]\n",
    "\n",
    "def drop_frequent_values(series, thresh=1):\n",
    "    value_counts = series.value_counts()\n",
    "    non_unique_values = value_counts[value_counts <= thresh].index\n",
    "    return series[series.isin(non_unique_values)]\n",
    "\n",
    "from utils import make_cv_scores_with_early_stopping, TargetMeanClassifier\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores_with_early_stopping = make_cv_scores_with_early_stopping(\"binary\", roc_auc_score, cv, early_stopping_rounds=20)\n",
    "\n",
    "class TargetMeanClassifierCut(TargetMeanClassifier):\n",
    "    def __init__(self, q_thresh=0):\n",
    "        super().__init__()\n",
    "        self.q_thresh = q_thresh\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_use = X.copy()\n",
    "        self.c_map = dict(X_use.value_counts())\n",
    "        self.c_map = {k: k  if v > self.q_thresh else 'nan' for k, v in self.c_map.items()}  # only keep those with more than 5 occurrences\n",
    "        X_use = X_use.map(self.c_map)\n",
    "        return super().fit(X_use, y)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X_use = X.copy()\n",
    "        X_use = X_use.map(self.c_map)\n",
    "        return super().predict_proba(X_use)\n",
    "\n",
    "class TargetMeanRegressorCut(TargetMeanRegressor):\n",
    "    def __init__(self, q_thresh=0):\n",
    "        super().__init__()\n",
    "        self.q_thresh = q_thresh\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # TODO: Make sure that the category dtype of train and val matches\n",
    "        X_use = X.copy()\n",
    "        self.c_map = dict(X_use.value_counts())\n",
    "        self.c_map = {k: k  if v > self.q_thresh else 'nan' for k, v in self.c_map.items()}  # only keep those with more than 5 occurrences\n",
    "        X_use = X_use.map(self.c_map)\n",
    "        return super().fit(X_use, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_use = X.copy()\n",
    "        X_use = X_use.map(self.c_map)\n",
    "        return super().predict(X_use)\n",
    "\n",
    "\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "class LightGBMClassifierCut(LGBMClassifier):\n",
    "    def __init__(self, q_thresh=0, init_kwargs: dict=dict()):\n",
    "        super().__init__(**init_kwargs)\n",
    "        self.verbose = init_kwargs.get(\"verbose\", -1)\n",
    "        self.n_estimators = init_kwargs.get(\"n_estimators\", 100)\n",
    "        self.max_bin = init_kwargs.get(\"max_bin\", 255)\n",
    "        self.max_depth = init_kwargs.get(\"max_depth\", 2)\n",
    "        self.random_state = init_kwargs.get(\"random_state\", 42)\n",
    "        self.init_kwargs = None\n",
    "        self.q_thresh = q_thresh\n",
    "\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        X_use = X.iloc[:,0].copy()\n",
    "        if X_use.dtype in ['object', 'category']:\n",
    "            fill = 'nan'\n",
    "        else:\n",
    "            fill = np.nan\n",
    "        self.c_map = dict(X_use.value_counts())\n",
    "        self.c_map = {k: k  if v > self.q_thresh else fill for k, v in self.c_map.items()}  # only keep those with more than 5 occurrences\n",
    "        X_use = X_use.map(self.c_map)\n",
    "        if X_use.dtype in ['object', 'category']:\n",
    "            X_use = X_use.astype('category')\n",
    "            self.dt = X_use.dtype \n",
    "        return super().fit(X_use.to_frame(), y, **kwargs)\n",
    "    \n",
    "    def predict_proba(self, X, **kwargs):\n",
    "        X_use = X.iloc[:,0].copy()\n",
    "        X_use = X_use.map(self.c_map)\n",
    "        if X_use.dtype in ['object', 'category']:   \n",
    "            X_use = X_use.astype(self.dt)\n",
    "        return super().predict_proba(X_use.to_frame(), **kwargs)\n",
    "\n",
    "class LightGBMRegressorCut(LGBMRegressor):\n",
    "    def __init__(self, q_thresh=0, init_kwargs: dict=dict()):\n",
    "        super().__init__(**init_kwargs)\n",
    "        self.verbose = init_kwargs.get(\"verbose\", -1)\n",
    "        self.n_estimators = init_kwargs.get(\"n_estimators\", 100)\n",
    "        self.max_bin = init_kwargs.get(\"max_bin\", 255)\n",
    "        self.max_depth = init_kwargs.get(\"max_depth\", 2)\n",
    "        self.random_state = init_kwargs.get(\"random_state\", 42)\n",
    "        self.init_kwargs = None\n",
    "        self.q_thresh = q_thresh\n",
    "\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        X_use = X.iloc[:,0].copy()\n",
    "        if X_use.dtype in ['object', 'category']:\n",
    "            fill = 'nan'\n",
    "        else:\n",
    "            fill = np.nan\n",
    "        self.c_map = dict(X_use.value_counts())\n",
    "        self.c_map = {k: k  if v > self.q_thresh else fill for k, v in self.c_map.items()}  # only keep those with more than 5 occurrences\n",
    "        X_use = X_use.map(self.c_map)\n",
    "        if X_use.dtype in ['object', 'category']:\n",
    "            X_use = X_use.astype('category')\n",
    "            self.dt = X_use.dtype \n",
    "        return super().fit(X_use.to_frame(), y, **kwargs)\n",
    "\n",
    "    def predict(self, X, **kwargs):\n",
    "        X_use = X.iloc[:,0].copy()\n",
    "        X_use = X_use.map(self.c_map)\n",
    "        if X_use.dtype in ['object', 'category']:   \n",
    "            X_use = X_use.astype(self.dt)\n",
    "        return super().predict(X_use.to_frame(), **kwargs)\n",
    "    \n",
    "def safe_stratified_group_kfold(X, y, groups, n_splits=5, max_attempts=1):\n",
    "    sgkf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    attempt = 0\n",
    "\n",
    "    while attempt < max_attempts:\n",
    "        for fold, (train_idx, test_idx) in enumerate(sgkf.split(X, y, groups)):\n",
    "            test_labels = y[test_idx]\n",
    "            class_counts = Counter(test_labels)\n",
    "            if len(class_counts) < 2:\n",
    "                break  # This fold has only one class\n",
    "        else:\n",
    "            return sgkf#.split(X, y, groups)  # All folds are good\n",
    "        \n",
    "        attempt += 1\n",
    "\n",
    "    print(\"Could not generate stratified group folds with both classes in all test sets.\")\n",
    "    return None  # If we reach here, it means we couldn't find a valid split\n",
    "\n",
    "def grouped_interpolation_test(x,y, target_type, add_dummy=False):\n",
    "    q = int(x.nunique())\n",
    "    \n",
    "    if target_type == 'binary':\n",
    "        # cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        cv = safe_stratified_group_kfold(x, y, x, n_splits=5)\n",
    "        if cv is None:\n",
    "            return None\n",
    "        cv_scores_with_early_stopping = make_cv_scores_with_early_stopping(\n",
    "            \"binary\", roc_auc_score, cv, early_stopping_rounds=20, \n",
    "            verbose=False, groups=x\n",
    "            )\n",
    "\n",
    "        lgb_model = LGBMClassifier(verbose=-1, n_estimators=q, random_state=42, max_bin=q, max_depth=2)\n",
    "    elif target_type == 'regression':\n",
    "        cv = GroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        cv_scores_with_early_stopping = make_cv_scores_with_early_stopping(\n",
    "            \"regression\", root_mean_squared_error, cv, early_stopping_rounds=20, \n",
    "            verbose=False, groups=x\n",
    "            )\n",
    "\n",
    "        lgb_model = LGBMRegressor(verbose=-1, n_estimators=q, random_state=42, max_bin=q, max_depth=2)\n",
    "\n",
    "\n",
    "    if add_dummy:\n",
    "        pipe = Pipeline([(\"model\", DummyRegressor())])\n",
    "        res = cv_scores_with_early_stopping(X, y, pipe)\n",
    "        print(f\"Dummy: {col}: {np.mean(res):.4f} (+/- {np.std(res):.4f})\")    \n",
    "\n",
    "    pipe = Pipeline([(\"model\", lgb_model)])\n",
    "    res = cv_scores_with_early_stopping(X[col].astype(float).to_frame(), y, pipe)\n",
    "    print(f\"{col}: {np.mean(res):.4f} (+/- {np.std(res):.4f})\")    \n",
    "    return res\n",
    "\n",
    "\n",
    "def analyze_cat_feature(x,y):\n",
    "    # Insight: histograms look different depending on whether we treat a feature as string or float - could somehow use this information\n",
    "    fig,ax = plt.subplots(1,4, figsize=(10, 5))\n",
    "    ax[0].scatter(x, y)\n",
    "\n",
    "    y_by_x = y.groupby(x, observed=False).mean()\n",
    "    ax[1].scatter(y_by_x.index, y_by_x.values)\n",
    "\n",
    "    ax[2].hist(x, bins=100)\n",
    "    pd.Series(x.astype(float).sort_values()).plot(kind='hist', bins=100, ax=ax[3])\n",
    "\n",
    "def get_feature_stats(x, y, target_type='binary', verbose=True):\n",
    "    q = int(x.nunique())\n",
    "    lgb_base_params = {\n",
    "        \"verbose\": -1, \"n_estimators\": q*10, \"random_state\": 42, \"max_bin\": q, \n",
    "        \"max_depth\": 2, \"min_samples_leaf\": 1, \"min_child_samples\": 1,\n",
    "    }\n",
    "    stats = {}\n",
    "    stats['q'] = q\n",
    "    if target_type == 'binary':\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        cv_scores_with_early_stopping = make_cv_scores_with_early_stopping(\n",
    "            \"binary\", roc_auc_score, cv, early_stopping_rounds=20, \n",
    "            verbose=False\n",
    "            )\n",
    "\n",
    "        lgb_class = lambda lgb_params: LGBMClassifier(**lgb_params)\n",
    "        target_model = TargetMeanClassifier()\n",
    "        target_cut_model = lambda t: TargetMeanClassifierCut(q_thresh=t)\n",
    "        lgb_cut_model = lambda t, p: LightGBMClassifierCut(q_thresh=t, init_kwargs=p)\n",
    "    elif target_type == 'regression':\n",
    "        cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        cv_scores_with_early_stopping = make_cv_scores_with_early_stopping(\n",
    "            \"regression\", root_mean_squared_error, cv, early_stopping_rounds=20, \n",
    "            verbose=False\n",
    "            )\n",
    "\n",
    "        lgb_class = lambda lgb_params: LGBMRegressor(**lgb_params)\n",
    "        target_model = TargetMeanRegressor()\n",
    "        target_cut_model = lambda t: TargetMeanRegressorCut(q_thresh=t)\n",
    "        lgb_cut_model = lambda t, p: LightGBMRegressorCut(q_thresh=t, init_kwargs=p)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported target type. Use 'binary' or 'regression'.\")\n",
    "\n",
    "\n",
    "    infreq = x.value_counts().sort_values(ascending=True).values[:5]\n",
    "\n",
    "    # Target-based stats\n",
    "    pipe = Pipeline([(\"model\", target_model)])\n",
    "    stats['mean'] = cv_scores_with_early_stopping(x.astype('category').to_frame(), y, pipe)\n",
    "    for t in infreq:\n",
    "        t = int(t)\n",
    "\n",
    "        pipe = Pipeline([(\"model\", target_cut_model(t))])\n",
    "        stats[f'mean-u>{t}'] = cv_scores_with_early_stopping(x.astype('category'), y, pipe)\n",
    "\n",
    "        pipe = Pipeline([(\"model\", target_model)])\n",
    "        x_use = drop_infrequent_values(x,t)\n",
    "        y_use = y.loc[x_use.index]\n",
    "        stats[f'mean-drop-u<={t}'] = cv_scores_with_early_stopping(x_use.astype('category'), y_use, pipe)\n",
    "\n",
    "        try:\n",
    "            pipe = Pipeline([(\"model\", target_model)])\n",
    "            x_use = drop_frequent_values(x,t)\n",
    "            y_use = y.loc[x_use.index]\n",
    "            stats[f'mean-drop-u>{t}'] = cv_scores_with_early_stopping(x_use.astype('category'), y_use, pipe)\n",
    "        except:\n",
    "            continue\n",
    "    # LGB-based stats\n",
    "    pipe = Pipeline([(\"model\", lgb_class(lgb_base_params))])\n",
    "    stats['lgb-num'] = cv_scores_with_early_stopping(x.astype(float).to_frame(), y, pipe)\n",
    "    pipe = Pipeline([(\"model\", lgb_class(lgb_base_params))])\n",
    "    stats['lgb-cat'] = cv_scores_with_early_stopping(x.astype('category').to_frame(), y, pipe)\n",
    "    for t in infreq:\n",
    "        t = int(t)\n",
    "        pipe = Pipeline([(\"model\", lgb_cut_model(t, lgb_base_params))])\n",
    "        stats[f'lgb-num-u>{t}'] = cv_scores_with_early_stopping(x.astype(float).to_frame(), y, pipe)\n",
    "\n",
    "        pipe = Pipeline([(\"model\", lgb_cut_model(t, lgb_base_params))])\n",
    "        stats[f'lgb-cat-u>{t}'] = cv_scores_with_early_stopping(x.astype('category').to_frame(), y, pipe)\n",
    "\n",
    "    # pipe = Pipeline([(\"model\", LightGBMClassifierCut(q_thresh=1, init_kwargs=lgb_base_params))])\n",
    "    # stats['lgb-num-u>1'] = cv_scores_with_early_stopping(x.astype(float).to_frame(), y, pipe)\n",
    "    # pipe = Pipeline([(\"model\", LightGBMClassifierCut(q_thresh=2, init_kwargs=lgb_base_params))])\n",
    "    # stats['lgb-num-u>2'] = cv_scores_with_early_stopping(x.astype(float).to_frame(), y, pipe)\n",
    "\n",
    "    # pipe = Pipeline([(\"model\", LightGBMClassifierCut(q_thresh=1, init_kwargs=lgb_base_params))])\n",
    "    # stats['lgb-cat-u>1'] = cv_scores_with_early_stopping(x.astype('category').to_frame(), y, pipe)\n",
    "    # pipe = Pipeline([(\"model\", LightGBMClassifierCut(q_thresh=2, init_kwargs=lgb_base_params))])\n",
    "    # stats['lgb-cat-u>2'] = cv_scores_with_early_stopping(x.astype('category').to_frame(), y, pipe)\n",
    "\n",
    "    # for n_est in [int(q/2),int(q/4), int(q/8)]:\n",
    "    #     p = lgb_base_params.copy()\n",
    "    #     p[\"n_estimators\"] = n_est\n",
    "    #     pipe = Pipeline([(\"model\", lgb_class(p))])\n",
    "    #     stats[f'lgb-{n_est}est-num'] = cv_scores_with_early_stopping(x.astype(float).to_frame(), y, pipe)\n",
    "    #     pipe = Pipeline([(\"model\", lgb_class(p))])\n",
    "    #     stats[f'lgb-{n_est}est-cat'] = cv_scores_with_early_stopping(x.astype('category').to_frame(), y, pipe)\n",
    "\n",
    "    if q>16:\n",
    "        for m_bin in [int(q/2),int(q/4), int(q/8), int(q/16)]:\n",
    "            # Cat features not affected \n",
    "            p = lgb_base_params.copy()\n",
    "            p[\"max_bin\"] = m_bin\n",
    "            pipe = Pipeline([(\"model\", lgb_class(p))])\n",
    "            stats[f'lgb-{m_bin}bins-num'] = cv_scores_with_early_stopping(x.astype(float).to_frame(), y, pipe)\n",
    "\n",
    "\n",
    "    # p = lgb_base_params.copy()\n",
    "    # p[\"max_depth\"] = 1\n",
    "    # pipe = Pipeline([(\"model\", lgb_class(p))])\n",
    "    # stats['lgb-d1-cat'] = cv_scores_with_early_stopping(x.astype('category').to_frame(), y, pipe)\n",
    "    # pipe = Pipeline([(\"model\", LightGBMClassifierCut(q_thresh=1, init_kwargs=p))])\n",
    "    # stats['lgb-d1-cat-u>1'] = cv_scores_with_early_stopping(x.astype('category').to_frame(), y, pipe)\n",
    "    # pipe = Pipeline([(\"model\", LightGBMClassifierCut(q_thresh=2, init_kwargs=p))])\n",
    "    # stats['lgb-d1-cat-u>2'] = cv_scores_with_early_stopping(x.astype('category').to_frame(), y, pipe)\n",
    "\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Feature: {x.name}\")\n",
    "        for key, value in stats.items():\n",
    "            if key in ['q']:\n",
    "                print(f\"{key}: {value}\")\n",
    "            else:\n",
    "                print(f\"{key}: {np.mean(value):.4f} (+/- {np.std(value):.4f})\")\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdb1f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_cat_thresh(x, y, target_type='binary', verbose=True):\n",
    "    q = int(x.nunique())\n",
    "    lgb_base_params = {\n",
    "        \"verbose\": -1, \"n_estimators\": q*10, \"random_state\": 42, \"max_bin\": q, \n",
    "        \"max_depth\": 2, \"min_samples_leaf\": 1, \"min_child_samples\": 1,\n",
    "    }\n",
    "    stats = {}\n",
    "    # stats['q'] = q\n",
    "    if target_type == 'binary':\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        cv_scores_with_early_stopping = make_cv_scores_with_early_stopping(\n",
    "            \"binary\", roc_auc_score, cv, early_stopping_rounds=20, \n",
    "            verbose=False\n",
    "            )\n",
    "\n",
    "        lgb_class = lambda lgb_params: LGBMClassifier(**lgb_params)\n",
    "        target_model = TargetMeanClassifier()\n",
    "        target_cut_model = lambda t: TargetMeanClassifierCut(q_thresh=t)\n",
    "        lgb_cut_model = lambda t, p: LightGBMClassifierCut(q_thresh=t, init_kwargs=p)\n",
    "    elif target_type == 'regression':\n",
    "        cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        cv_scores_with_early_stopping = make_cv_scores_with_early_stopping(\n",
    "            \"regression\", root_mean_squared_error, cv, early_stopping_rounds=20, \n",
    "            verbose=False\n",
    "            )\n",
    "\n",
    "        lgb_class = lambda lgb_params: LGBMRegressor(**lgb_params)\n",
    "        target_model = TargetMeanRegressor()\n",
    "        target_cut_model = lambda t: TargetMeanRegressorCut(q_thresh=t)\n",
    "        lgb_cut_model = lambda t, p: LightGBMRegressorCut(q_thresh=t, init_kwargs=p)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported target type. Use 'binary' or 'regression'.\")\n",
    "\n",
    "\n",
    "    infreq = x.value_counts().sort_values(ascending=True).unique()[:100]\n",
    "\n",
    "    # Target-based stats\n",
    "    pipe = Pipeline([(\"model\", target_model)])\n",
    "    stats['mean'] = cv_scores_with_early_stopping(x.astype('category').to_frame(), y, pipe)\n",
    "    for t in infreq:\n",
    "        t = int(t)\n",
    "\n",
    "        pipe = Pipeline([(\"model\", target_cut_model(t))])\n",
    "        stats[f'mean-u>{t}'] = cv_scores_with_early_stopping(x.astype('category'), y, pipe)\n",
    "\n",
    "        # pipe = Pipeline([(\"model\", target_model)])\n",
    "        # x_use = drop_infrequent_values(x,t)\n",
    "        # y_use = y.loc[x_use.index]\n",
    "        # stats[f'mean-drop-u<={t}'] = cv_scores_with_early_stopping(x_use.astype('category'), y_use, pipe)\n",
    "\n",
    "        # try:\n",
    "        #     pipe = Pipeline([(\"model\", target_model)])\n",
    "        #     x_use = drop_frequent_values(x,t)\n",
    "        #     y_use = y.loc[x_use.index]\n",
    "        #     stats[f'mean-drop-u>{t}'] = cv_scores_with_early_stopping(x_use.astype('category'), y_use, pipe)\n",
    "        # except:\n",
    "        #     continue\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Feature: {x.name}\")\n",
    "        for key, value in stats.items():\n",
    "            if key in ['q']:\n",
    "                print(f\"{key}: {value}\")\n",
    "            else:\n",
    "                print(f\"{key}: {np.mean(value):.4f} (+/- {np.std(value):.4f})\")\n",
    "\n",
    "    plt.plot(pd.DataFrame(stats).mean(axis=0))\n",
    "    plt.title(f\"Feature: {x.name}\")\n",
    "    plt.show()\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08370ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_stats(x, y, target_type='binary', verbose=True):\n",
    "    q = int(x.nunique())\n",
    "    lgb_base_params = {\n",
    "        \"verbose\": -1, \"n_estimators\": q*10, \"random_state\": 42, \"max_bin\": q, \n",
    "        \"max_depth\": 2, \"min_samples_leaf\": 1, \"min_child_samples\": 1,\n",
    "    }\n",
    "    stats = {}\n",
    "    stats['q'] = q\n",
    "    if target_type == 'binary':\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        cv_scores_with_early_stopping = make_cv_scores_with_early_stopping(\n",
    "            \"binary\", roc_auc_score, cv, early_stopping_rounds=20, \n",
    "            verbose=False\n",
    "            )\n",
    "\n",
    "        lgb_class = lambda lgb_params: LGBMClassifier(**lgb_params)\n",
    "        target_model = TargetMeanClassifier()\n",
    "        target_cut_model = lambda t: TargetMeanClassifierCut(q_thresh=t)\n",
    "    elif target_type == 'regression':\n",
    "        cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        cv_scores_with_early_stopping = make_cv_scores_with_early_stopping(\n",
    "            \"regression\", root_mean_squared_error, cv, early_stopping_rounds=20, \n",
    "            verbose=False\n",
    "            )\n",
    "\n",
    "        lgb_class = lambda lgb_params: LGBMRegressor(**lgb_params)\n",
    "        target_model = TargetMeanRegressor()\n",
    "        target_cut_model = lambda t: TargetMeanRegressorCut(q_thresh=t)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported target type. Use 'binary' or 'regression'.\")\n",
    "\n",
    "    # Target-based stats\n",
    "    pipe = Pipeline([(\"model\", target_model)])\n",
    "    stats['mean'] = cv_scores_with_early_stopping(x.astype('category').to_frame(), y, pipe)\n",
    "    for t in range(1,5):\n",
    "        pipe = Pipeline([(\"model\", target_cut_model(t))])\n",
    "        stats[f'mean-u>{t}'] = cv_scores_with_early_stopping(x.astype('category'), y, pipe)\n",
    "\n",
    "    for t in range(1,5):\n",
    "        pipe = Pipeline([(\"model\", target_model)])\n",
    "        x_use = drop_infrequent_values(x,t)\n",
    "        y_use = y.loc[x_use.index]\n",
    "        stats[f'mean-drop-l<={t}'] = cv_scores_with_early_stopping(x_use.astype('category'), y_use, pipe)\n",
    "    for t in range(1,5):\n",
    "        pipe = Pipeline([(\"model\", target_model)])\n",
    "        x_use = drop_frequent_values(x,t)\n",
    "        y_use = y.loc[x_use.index]\n",
    "        stats[f'mean-drop-l>{t}'] = cv_scores_with_early_stopping(x_use.astype('category'), y_use, pipe)\n",
    "\n",
    "    # LGB-based stats\n",
    "    pipe = Pipeline([(\"model\", lgb_class(lgb_base_params))])\n",
    "    stats['lgb-num'] = cv_scores_with_early_stopping(x.astype(float).to_frame(), y, pipe)\n",
    "    pipe = Pipeline([(\"model\", lgb_class(lgb_base_params))])\n",
    "    stats['lgb-cat'] = cv_scores_with_early_stopping(x.astype('category').to_frame(), y, pipe)\n",
    "    for t in range(5):\n",
    "        pipe = Pipeline([(\"model\", LightGBMClassifierCut(q_thresh=t, init_kwargs=lgb_base_params))])\n",
    "        stats[f'lgb-num-u>{t}'] = cv_scores_with_early_stopping(x.astype(float).to_frame(), y, pipe)\n",
    "\n",
    "        pipe = Pipeline([(\"model\", LightGBMClassifierCut(q_thresh=t, init_kwargs=lgb_base_params))])\n",
    "        stats[f'lgb-cat-u>{t}'] = cv_scores_with_early_stopping(x.astype('category').to_frame(), y, pipe)\n",
    "\n",
    "    # pipe = Pipeline([(\"model\", LightGBMClassifierCut(q_thresh=1, init_kwargs=lgb_base_params))])\n",
    "    # stats['lgb-num-u>1'] = cv_scores_with_early_stopping(x.astype(float).to_frame(), y, pipe)\n",
    "    # pipe = Pipeline([(\"model\", LightGBMClassifierCut(q_thresh=2, init_kwargs=lgb_base_params))])\n",
    "    # stats['lgb-num-u>2'] = cv_scores_with_early_stopping(x.astype(float).to_frame(), y, pipe)\n",
    "\n",
    "    # pipe = Pipeline([(\"model\", LightGBMClassifierCut(q_thresh=1, init_kwargs=lgb_base_params))])\n",
    "    # stats['lgb-cat-u>1'] = cv_scores_with_early_stopping(x.astype('category').to_frame(), y, pipe)\n",
    "    # pipe = Pipeline([(\"model\", LightGBMClassifierCut(q_thresh=2, init_kwargs=lgb_base_params))])\n",
    "    # stats['lgb-cat-u>2'] = cv_scores_with_early_stopping(x.astype('category').to_frame(), y, pipe)\n",
    "\n",
    "    # for n_est in [int(q/2),int(q/4), int(q/8)]:\n",
    "    #     p = lgb_base_params.copy()\n",
    "    #     p[\"n_estimators\"] = n_est\n",
    "    #     pipe = Pipeline([(\"model\", lgb_class(p))])\n",
    "    #     stats[f'lgb-{n_est}est-num'] = cv_scores_with_early_stopping(x.astype(float).to_frame(), y, pipe)\n",
    "    #     pipe = Pipeline([(\"model\", lgb_class(p))])\n",
    "    #     stats[f'lgb-{n_est}est-cat'] = cv_scores_with_early_stopping(x.astype('category').to_frame(), y, pipe)\n",
    "\n",
    "\n",
    "    for m_bin in [int(q/2),int(q/4), int(q/8), int(q/16)]:\n",
    "        # Cat features not affected \n",
    "        p = lgb_base_params.copy()\n",
    "        p[\"max_bin\"] = m_bin\n",
    "        pipe = Pipeline([(\"model\", lgb_class(p))])\n",
    "        stats[f'lgb-{m_bin}bins-num'] = cv_scores_with_early_stopping(x.astype(float).to_frame(), y, pipe)\n",
    "\n",
    "\n",
    "    # p = lgb_base_params.copy()\n",
    "    # p[\"max_depth\"] = 1\n",
    "    # pipe = Pipeline([(\"model\", lgb_class(p))])\n",
    "    # stats['lgb-d1-cat'] = cv_scores_with_early_stopping(x.astype('category').to_frame(), y, pipe)\n",
    "    # pipe = Pipeline([(\"model\", LightGBMClassifierCut(q_thresh=1, init_kwargs=p))])\n",
    "    # stats['lgb-d1-cat-u>1'] = cv_scores_with_early_stopping(x.astype('category').to_frame(), y, pipe)\n",
    "    # pipe = Pipeline([(\"model\", LightGBMClassifierCut(q_thresh=2, init_kwargs=p))])\n",
    "    # stats['lgb-d1-cat-u>2'] = cv_scores_with_early_stopping(x.astype('category').to_frame(), y, pipe)\n",
    "\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Feature: {x.name}\")\n",
    "        for key, value in stats.items():\n",
    "            if key in ['q']:\n",
    "                print(f\"{key}: {value}\")\n",
    "            else:\n",
    "                print(f\"{key}: {np.mean(value):.4f} (+/- {np.std(value):.4f})\")\n",
    "\n",
    "    # return stats\n",
    "\n",
    "get_feature_stats(X['RESOURCE'], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0abb93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d57b7fe",
   "metadata": {},
   "source": [
    "### Remaining issues\n",
    "- Still false positives on datasets with many features like \n",
    "    - APSFailure: ['ae_000', 'af_000', 'ak_000', 'ar_000', 'as_000', 'au_000', 'cf_000', 'cz_000', 'da_000', 'db_000', 'df_000', 'dh_000', 'dj_000', 'dk_000', 'dl_000', 'dr_000', 'dy_000', 'dz_000', 'ea_000', 'ef_000', 'eg_000']\n",
    "    - Bioresponse: ['D2', 'D14', 'D31', 'D32', 'D33', 'D35', 'D36', 'D38', 'D39', 'D40', 'D44', 'D45', 'D46', 'D48', 'D49', 'D53', 'D54', 'D55', 'D56', 'D57', 'D58', 'D60', 'D61', 'D62', 'D63', 'D64', 'D65', 'D67', 'D68', 'D69', 'D71', 'D74', 'D75', 'D76', 'D87', 'D95', 'D103', 'D547', 'D754', 'D771', 'D866']\n",
    "- Ordinals like age often found to be categorical \n",
    "    - (bank & house datasets)\n",
    "    - Bank_Customer_Churn: age\n",
    "    - airline satisfaction: Age\n",
    "    - miami_housing: age\n",
    "\n",
    "\n",
    "- On churn we find state to be numeric (which might be a good thing)\n",
    "- On MarketingCampaign some new categoricals are only identified as such with AG preprocessing: ['Year_Birth', 'NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth', 'Dt_Customer.month']\n",
    "- Several other FPs (likely mostly ordinals)\n",
    "    - coil: ['contributionFirePolicies', 'numberOfFirePolicies']\n",
    "    - concrete: ['BlastFurnaceSlag', 'Water', 'CoarseAggregate', 'FineAggregate', 'Age']\n",
    "    - credit default: ['BILL_AMT1']\n",
    "    - maternal_health_risk: ['Age', 'SystolicBP', 'BS', 'BodyTemp', 'HeartRate']\n",
    "    - online_shoppers_intention: ['Informational', 'SpecialDay']\n",
    "    - students dropout: ['Previous_qualification_grade', 'Curricular_units_1st_sem_enrolled', 'Curricular_units_1st_sem_evaluations', 'Curricular_units_2nd_sem_enrolled', 'Curricular_units_2nd_sem_approved']\n",
    "    - MIC: ['NA_BLOOD']\n",
    "- Several FPs on the special chemistry datasets\n",
    "    - physio: ['TotalSurfaceArea', 'NonPolarExposedArea', 'MassWeightedExposedArea', 'EuclideanDistance']\n",
    "    - qsar-biodeg: ['Laplace_MoharIndex2']\n",
    "    - superconduct: almost all\n",
    "- AG does quite often find new numerics!!\n",
    "\n",
    "\n",
    "\n",
    "Positive:\n",
    "- Almost no new numeric!\n",
    "    - state in churn (good thing likely)\n",
    "    - airline: ['DepartureArrivaltimeconvenient', 'Foodanddrink', 'Inflightwifiservice', 'Inflightentertainment', 'EaseofOnlinebooking', 'Onboardservice', 'Checkinservice', 'Cleanliness']\n",
    "    - online shoppers intention: ['OperatingSystems']\n",
    "    - qsar-biodeg: ['C026_chemical_substructure']\n",
    "- Many clear numerics are solved\n",
    "\n",
    "\n",
    "Possible additional tests:\n",
    "- Interaction test\n",
    "    - Check whether features interact linearly with other features, which is another sign of being numerical\n",
    "\n",
    "- LOO vs. univariate linear regression - if LOO performs better, we have a clear pattern.\n",
    "    - Would certainly rule out several features immediately that we havent covered yet, i.e. in anneal dataset. Also one uninformative featue in Amazon dataset\n",
    "\n",
    "- Unique removal test\n",
    "    - Do something about unique values - i.e. a test that finds whether we can infer information for them by using the feature as numeric\n",
    "- stratified CV using the feature as target: \n",
    "    - Perfect condition for mean scenario, if it still can be beaten, we likely have a numeric feature\n",
    "\n",
    "Tried: \n",
    "- performance test: Rule out the features interacting with the target in a complex way using LGB, as previously\n",
    "    - Interestingly is always the same as after the previous tests - it is impossible to find something new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2fd49a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3e5fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ca7186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2d0c8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98597d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ad17c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c6132c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
